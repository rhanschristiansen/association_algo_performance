{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "association_test.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kylehounslow/association_algo_performance/blob/master/src/association_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6d1IqVaDZZa",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "## The Association Process Testing\n",
        "\n",
        "This file documents the process of performing association between lidar reading and detected bounding boxes of objects in the feild of view of both a video camera and a LeddarTech M16 Lidar Detector.\n",
        "\n",
        "#### Background\n",
        "\n",
        "__LIDAR Detector__\n",
        "\n",
        "The LeddarTech M16 provides a non-scanning lidar measurement which returns high sampling speed distance measurements from 16 lidar zones that are arrayed from left to right in a 45 degree field of view horizontally and 7.5 degrees vertically. The individual segments cover equal angles of about 2.8 degrees horizontally and 7.5 degrees vertically. Each LeddarTech reading consists of a segment number (0-15) and a distance reading. There may be zero, one or more distance measurements per segment depending on objects that are in the field of view. The range of the M16 is about 140 feet.\n",
        "\n",
        "__Video Object Detector__\n",
        "\n",
        "The setup uses the Tensorflow Deep Learning software package https://www.tensorflow.org/ and the YOLOv3 model https://github.com/maiminh1996/YOLOv3-tensorflow to detect objects in the field of view of the camera.\n",
        "\n",
        "__Association__\n",
        "\n",
        "having both distance and object detection information available is extremely valuable but it is highly important that we know which lidar distance reading is associated with which object returned from the video object detector. This is __The Association Problem__ that is developed and tested in this notebook.\n",
        "\n",
        "\n",
        "#### Description of the Problem\n",
        "\n",
        "The drawing below shows a 3d Projection of the LeddarTech M16 field of view onto the field of view of the video camera.\n",
        "\n",
        "<html><img src=https://github.com/rhanschristiansen/association_algo_performance/blob/master/src/images/Fields_of_View.jpg?raw=1 width=560></html>\n",
        "\n",
        "When the setup is deployed in the field, the information that is retrieved for a single frame looks like the image below. Here you can see the lidar zones of the M16 shown with thin black lines across the lower middle of the image. \n",
        "\n",
        "__Displaying Lidar Readings__\n",
        "\n",
        "If a zone of the lidar detector returns a distance reading the zone is highlighted in yellow and the distance reading (in feet) is displayed above the lidar zone. If more than one value is returned for a given zone, multiple red distance readings are stacked vertically over the lidar zone. \n",
        "\n",
        "__Displaying Video Detections__\n",
        "\n",
        "The object detections that are returned from the TensorFlow Yolo detector are shown as green bounding boxes. The class of the object are also shown in green text in the lower left corner of the bounding box.\n",
        "\n",
        "In the frame below, there were 5 lidar distance readings returned in zones 4, 5, 6, 7 & 8. And there were 3 objects detected. In this scenario, a fairly simple algorithm could be developed to map lidar values to objects.\n",
        "\n",
        "These are the video detection objects:\n",
        "\n",
        "|Object # | Bounding Box (x1, y1, x2, y2) | Object Class | Confidence |\n",
        "| :-----: | :---------------------------: | :----------: | :--------: |\n",
        "|    1    |   (495, 354, 561, 409)        |    Car       |  0.9839655 |\n",
        "|    2    |   (663, 366, 697, 392)        |    Car       |  0.980555  |\n",
        "|    3    |   (598, 368, 667, 420)        |    Car       |  0.9420464 |\n",
        "\n",
        "And these are the lidar detections:\n",
        "\n",
        "|Detection # | Segment |   Distance   |\n",
        "| :--------: | :-----: | :----------: | \n",
        "|    1       |    4    | 105.799030   |\n",
        "|    2       |    5    | 105.018769   |\n",
        "|    3       |    6    | 104.506889   |\n",
        "|    4       |    7    |  88.595796   |\n",
        "|    5       |    8    |  88.714592   |\n",
        "\n",
        "<html><img src=https://github.com/rhanschristiansen/association_algo_performance/blob/master/src/images/lidar_to_image_rendering_14.png?raw=1 width=1280></html>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssk3Gb3WDZZc",
        "colab_type": "text"
      },
      "source": [
        "However, in other situations, the situation is much more complex. Observe the complexity of the situation when more objects and lidar readings are detected. In this frame shown below, there are a total of 8 objects and 23 lidar detections. This presents a significant challenge to any association algorithm.\n",
        "\n",
        "In this frame, these are the video detection objects:\n",
        "\n",
        "|Object # | Bounding Box (x1, y1, x2, y2) | Object Class | Confidence  |\n",
        "| :-----: | :---------------------------: | :----------: | :---------: |\n",
        "|    1    |   ( -7, 317, 427, 621)        |    Car       |  0.9997973  |\n",
        "|    2    |   (786, 354, 1027, 504)       |    Car       |  0.9997645  |\n",
        "|    3    |   (380, 360, 512, 454)        |    Car       |  0.99277544 |\n",
        "|    4    |   (1129, 418, 1270, 713)      |    Car       |  0.9813789  |\n",
        "|    5    |   (589, 371, 669, 420)        |    Car       |  0.97814137 |\n",
        "|    6    |   (741, 363, 825, 440)        |    Car       |  0.9496468  |\n",
        "|    7    |   (707, 372, 738, 407)        |    Car       |  0.8582622  |\n",
        "|    8    |   (517, 355, 578, 410)        |    Car       |  0.8155548  |\n",
        "\n",
        "And these are the lidar detections:\n",
        "\n",
        "|Detection # | Segment |   Distance   |\n",
        "| :--------: | :-----: | :----------: | \n",
        "|    1       |    0    |   20.352862  |\n",
        "|    2       |    3    |   51.171712  |\n",
        "|    3       |    4    |   50.816675  |\n",
        "|    4       |    5    |   50.729818  |\n",
        "|    5       |    5    |   91.829979  |\n",
        "|    6       |    6    |   91.648105  |\n",
        "|    7       |    7    |   85.183094  |\n",
        "|    8       |    7    |  112.802604  |\n",
        "|    9       |    8    |   84.882674  |\n",
        "|   10       |    8    |  111.584855  |\n",
        "|   11       |    9    |   84.740849  |\n",
        "|   12       |    9    |  112.994140  |\n",
        "|   13       |   10    |   59.250310  |\n",
        "|   14       |   10    |   84.906053  |\n",
        "|   15       |   11    |   59.656511  |\n",
        "|   16       |   12    |   34.480209  |\n",
        "|   17       |   13    |   35.184426  |\n",
        "|   18       |   13    |  108.944754  |\n",
        "|   19       |   14    |   35.332408  |\n",
        "|   20       |   14    |  108.647988  |\n",
        "|   21       |   15    |   35.796530  |\n",
        "|   22       |   14    |   37.425736  |\n",
        "|   23       |   14    |  110.845094  |\n",
        "\n",
        "<html><img src=https://github.com/rhanschristiansen/association_algo_performance/blob/master/src/images/lidar_to_image_rendering_19.png?raw=1 width=1280></html>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVbowpnyDZZd",
        "colab_type": "text"
      },
      "source": [
        "### Solving the Association Problem\n",
        "\n",
        "The image below shows a close up of the simplest association problem so that we can outine an algorithm to address the solution.\n",
        "\n",
        "<html><img src=https://github.com/rhanschristiansen/association_algo_performance/blob/master/src/images/closeup_association.png?raw=1 width=560></html>\n",
        "\n",
        "These are the video detection objects (in green):\n",
        "\n",
        "|Object # | Bounding Box (x1, y1, x2, y2) | Object Class | Confidence |\n",
        "| :-----: | :---------------------------: | :----------: | :--------: |\n",
        "|    1    |   (495, 354, 561, 409)        |    Car       |  0.9839655 |\n",
        "|    2    |   (663, 366, 697, 392)        |    Car       |  0.980555  |\n",
        "|    3    |   (598, 368, 667, 420)        |    Car       |  0.9420464 |\n",
        "\n",
        "From left to right the three objects are 1, 3 & 2 \n",
        "\n",
        "And these are the lidar detections (in red):\n",
        "\n",
        "|Detection # | Segment |   Distance   |\n",
        "| :--------: | :-----: | :----------: | \n",
        "|    1       |    4    | 105.799030   |\n",
        "|    2       |    5    | 105.018769   |\n",
        "|    3       |    6    | 104.506889   |\n",
        "|    4       |    7    |  88.595796   |\n",
        "|    5       |    8    |  88.714592   |\n",
        "\n",
        "From left to right the lidar detections are in numerical order 1, 2, 3, 4 & 5 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHDYS3YODZZd",
        "colab_type": "text"
      },
      "source": [
        "From the data above, using our intuition we could make the following proposals for rules:\n",
        "\n",
        ">1) A lidar reading can only be associated with the object if the object bounding box intersects the segment of the lidar region\n",
        "\n",
        "If we represent an associations matrix with objects in rows and lidar detections in columns and with ones in position (i,j) representing an association between object i and lidar detection j. \n",
        "\n",
        "Using rule #1 we would have the following associations matrix:\n",
        "\n",
        "| Obj# \\ Det# |  1  |  2  |  3  |  4  |  5  |\n",
        "| ----------: | :-: | :-: | :-: | :-: | :-: | \n",
        "|      1      |  0  |  1  |  1  |  0  |  0  |\n",
        "|      2      |  0  |  0  |  0  |  0  |  1  |\n",
        "|      3      |  0  |  0  |  0  |  1  |  1  |\n",
        "\n",
        "Unfortunately, objects 1 and 3 both have two different lidar readings associated with them and lidar reading 5 is associated with two different objects.\n",
        "\n",
        "For the second case, shown above, the association matrix would be significantly more complex:\n",
        "\n",
        "| Obj# \\ Det# |  1  |  2  |  3  |  4  |  5  |  6  |  7  |  8  |  9  | 10  | 11  | 12  | 13  | 14  | 15  | 16  | 17  | 18  | 19  | 20  | 21  | 22  | 23  |\n",
        "| ----------: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | \n",
        "|      1      |  1  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |\n",
        "|      2      |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  1  |  1  |  1  |  1  |  1  |  1  |  1  |  1  |  1  |  1  |  1  |\n",
        "|      3      |  0  |  1  |  1  |  1  |  1  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |\n",
        "|      4      |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |\n",
        "|      5      |  0  |  0  |  0  |  0  |  0  |  0  |  1  |  1  |  1  |  1  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |\n",
        "|      6      |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  1  |  1  |  1  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |\n",
        "|      7      |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  1  |  1  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |\n",
        "|      8      |  0  |  0  |  0  |  1  |  1  |  1  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8otLGtvDZZe",
        "colab_type": "text"
      },
      "source": [
        "In this case we have most objects with more than one lidar reading associated with them and several lidar readings that are associated with more than one object.\n",
        "\n",
        "One approach that could be attempted is to develop a rules based approach for choosing the best lidar reading for each individual object. Once possible rule might be:\n",
        "\n",
        ">2) When more than one lidar reading is associated with an object based on rule #1 above, choose the lidar reading with the closest distance.\n",
        "\n",
        "Unfortunately, using this rule leads to an conflict with objects 3 and 8 that are both associated with lidar reading #4 using rule 2 which has the lowest distance. By observation, it is clear that lidar reading #4 should be associated with object #8 rather than object #2 because object #8 is the closer object that is closer than object #2.\n",
        "\n",
        "By trying to create a rules based approach to solve the dilemma, we can run into an endless set of rules that are complex and self-contradictory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gZcHtqODZZf",
        "colab_type": "text"
      },
      "source": [
        "### The Hungarian Algorithm\n",
        "\n",
        "One of the methods to solve this assignment problem is the __Hungarian Algorithm__ http://hungarianalgorithm.com/index.php or one of its variants called the __Munkres Algorithm__ http://csclab.murraystate.edu/~bob.pilgrim/445/munkres.html. The Munk-Res algorithm is similar to the Hungarian algorithm but works with non-square matrices.\n",
        "\n",
        "\n",
        "#### The cost function\n",
        "\n",
        "The input to the Munkres Algorithm is a cost function matrix which defines a __cost__ value for each object/lidar detection assignment pair. The Munkres Algotithm will return an assignment matrix or assignment pairs where the overall sum of the costs for all the assignments is minimized. \n",
        "\n",
        "To make the algorithm work it is necessary to create a cost function that decreases as the quality of the assignment increases.\n",
        "\n",
        "#### The Ideal Bounding Box\n",
        "\n",
        "__Possible Cost Functions__\n",
        "\n",
        "Several possible cost functions can be developed for the input to the Munkres Algorithm\n",
        "\n",
        ">1) The L2 Norm - "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJoMRstiEbWk",
        "colab_type": "text"
      },
      "source": [
        "# Colab Environment Setup\n",
        "Download the supporting source files and data to the Colab environment\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsiI3iZUEyb9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clone_source_code():\n",
        "  \"\"\"\n",
        "  Clone the github repo and move to this working directory\n",
        "  \"\"\"\n",
        "  print(\"Downloading source code...\")\n",
        "#   !_=$(git clone --quiet https://github.com/rhanschristiansen/association_algo_performance.git)\n",
        "  !_=$(git clone --quiet https://github.com/kylehounslow/association_algo_performance.git)\n",
        "  !mv association_algo_performance/* .\n",
        "  !rm -rf association_algo_performance/\n",
        "\n",
        "def download_extract_data():\n",
        "  \"\"\"\n",
        "  Download data.zip from Google Drive and extract to this working directory\n",
        "  \"\"\"\n",
        "  print(\"Downloading data...\")\n",
        "  !curl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id=1ornFmw59u_It0Cpi8yDHRdpW4G_6YLj5\" > /dev/null\n",
        "  !curl -s -Lb ./cookie \"https://drive.google.com/uc?export=download&confirm=`awk '/download/ {print $NF}' ./cookie`&id=1ornFmw59u_It0Cpi8yDHRdpW4G_6YLj5\" -o \"data.zip\"\n",
        "  print(\"Extracting data...\")\n",
        "  !unzip -q data.zip\n",
        "def download_yolov3_weights():\n",
        "  print(\"Downloading CNN weights...\")\n",
        "  !curl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id=1pdFmnxLEbezktEbA7tfnVoU50wowXI2r\" > /dev/null\n",
        "  !curl -s -Lb ./cookie \"https://drive.google.com/uc?export=download&confirm=`awk '/download/ {print $NF}' ./cookie`&id=1pdFmnxLEbezktEbA7tfnVoU50wowXI2r\" -o \"src/detection/yolov3/yolov3.weights\"\n",
        "\n",
        "def install_requirements():\n",
        "  print(\"Installing pip packages...\")\n",
        "  !pip install --quiet -r src/requirements.txt\n",
        "\n",
        "def setup():\n",
        "  import sys\n",
        "  sys.path.append('./') #add the parent directory to the path\n",
        "  !rm -rf *\n",
        "  clone_source_code()\n",
        "  install_requirements()\n",
        "  download_extract_data()\n",
        "  download_yolov3_weights()\n",
        "  print(\"Setup Complete.\")\n",
        "\n",
        "setup()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huNkCbayQ81J",
        "colab_type": "text"
      },
      "source": [
        "# The Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RkTDehVTAUr",
        "colab_type": "text"
      },
      "source": [
        "## Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "code_folding": [],
        "id": "83qgSgV9DZae",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def draw_bboxes(bboxes, img):\n",
        "    \"\"\"\n",
        "    Draw bounding boxes to frame\n",
        "    :param bboxes: list of bboxes in [x1,y1,x2,y2] format\n",
        "    :param img: np.array\n",
        "    :return: image with bboxes drawn\n",
        "    \"\"\"\n",
        "    img = img.copy()\n",
        "    if bboxes is not None and len(bboxes) > 0:\n",
        "        for i, bb in enumerate(bboxes):\n",
        "            cv2.rectangle(img, (bb[0], bb[1]), (bb[2], bb[3]), (0, 255, 0), 2)\n",
        "\n",
        "    return img\n",
        "\n",
        "def play_video_html(video_filepath: str, width: int = 720):\n",
        "    from IPython.display import HTML\n",
        "    from base64 import b64encode\n",
        "    mp4 = open(video_filepath,'rb').read()\n",
        "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "    return HTML(\"\"\"\n",
        "    <video width={width} controls autoplay>\n",
        "        <source src=\"{data_url}\" type=\"video/mp4\">\n",
        "    </video>\n",
        "    \"\"\" .format(width=width, data_url=data_url))\n",
        "\n",
        "def download_file(filepath: str):\n",
        "    from google.colab import files\n",
        "    files.download(filepath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pT1DfwHTHh3",
        "colab_type": "text"
      },
      "source": [
        "## Association Test Runner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "code_folding": [],
        "id": "2NJ4eb8EDZai",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This is the main loop for processing and testing associations\n",
        "\n",
        "def run_association_test(manual_test, output_video_file:str = \"\", read_file:bool = False):\n",
        "\n",
        "    # This contains all of the imports needed for running the association tests\n",
        "\n",
        "    import os\n",
        "    import cv2\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "    from src.detection.car_detector_tf_v2 import CarDetectorTFV2\n",
        "    from src.detection.detection import Detection\n",
        "    from src.lidar.lidar_detection import LIDAR_detection\n",
        "    from src.association.association import Association\n",
        "    from src.association.costs import Costs\n",
        "    import src.util.calibration_kitti as cal\n",
        "    import math\n",
        "    import pykitti\n",
        "    import datetime\n",
        "    from tqdm.notebook import tqdm\n",
        "    \n",
        "    \n",
        "    # This contains all of the defines and hyperparameters neededfor the associations tests\n",
        "    video_writer = None\n",
        "    WRITE_DATA_FILE = False\n",
        "\n",
        "    # set USE_DETECTOR to True to use the Yolo CNN or False to use the ground truth for the Video Detections\n",
        "    USE_DETECTOR = True\n",
        "    # the minimum detection confidence level for the Yolo CNN\n",
        "    CONFIDENCE_THRESHOLD = 0.8\n",
        "\n",
        "\n",
        "    # the run control variables\n",
        "    PAUSE = False\n",
        "    DISP_LIDAR = False\n",
        "    DISP_DET = False\n",
        "    DISP_ASSOC = True\n",
        "    DISP_ZONES = True\n",
        "    DISP_TRUTH = True\n",
        "    DISP_RESULTS = True\n",
        "    SLOW = False\n",
        "  \n",
        "    global x_pixel, y_pixel, no_mouse_click_count\n",
        "    x_pixel = -1\n",
        "    y_pixel = -1\n",
        "    max_no_mouse_click_count = 100\n",
        "    no_mouse_click_count = max_no_mouse_click_count\n",
        "\n",
        "    def mouse_click(event, x, y, flags, param):\n",
        "        global x_pixel, y_pixel, no_mouse_click_count\n",
        "        no_mouse_click_count = max_no_mouse_click_count\n",
        "\n",
        "        if event == cv2.EVENT_MOUSEMOVE:\n",
        "            x_pixel = x\n",
        "            y_pixel = y\n",
        "    \n",
        "    # the locations and dates of the video and lidat data files\n",
        "    PWD = './'\n",
        "    DATA_DATE = '2011_09_26'\n",
        "    RUN_NUMBER = '0015'\n",
        "    DATA_DIR = './data'\n",
        "\n",
        "    video_frame_lag = 0\n",
        "    dist_thresh = 0.2 # set higher to allow less accurate lidar readings to be labeled as correct\n",
        "\n",
        "    # This contains the setup for the lidar detections\n",
        "\n",
        "    lidar_left = cal.SEG_TO_PIXEL_LEFT[0]\n",
        "    lidar_right = cal.SEG_TO_PIXEL_RIGHT[15]\n",
        "    lidar_top = cal.SEG_TO_PIXEL_TOP\n",
        "    lidar_bottom = cal.SEG_TO_PIXEL_BOTTOM\n",
        "\n",
        "    # read the lidar data\n",
        "    m16 = pd.read_csv('{}/{}/{}_filtered.csv'.format(DATA_DIR, DATA_DATE, RUN_NUMBER ), skiprows=2)\n",
        "\n",
        "    # read the ground truth from the tracklets data\n",
        "    gt_df = pd.read_csv('{}/{}'.format(DATA_DIR, '2011_09_26_drive_0015_sync_converted-tracklets.csv'))\n",
        "    gt_df['dist'] = gt_df['dist'] * cal.cal['M_TO_FT']\n",
        "    # remove all objects that are not vehicles\n",
        "    gt_df = gt_df[(gt_df['label']=='Car') | (gt_df['label']=='Truck') | (gt_df['label']=='Van')]\n",
        "    # remove all objects outside the range of the lidar detector\n",
        "    gt_df = gt_df[(gt_df['dist']>=30) & (gt_df['dist'] <= 140)]\n",
        "    #remove all objects outside of the lidar fov\n",
        "    gt_df = gt_df[gt_df['x1'] <= lidar_right]\n",
        "    gt_df = gt_df[gt_df['x2'] >= lidar_left]\n",
        "    gt_df = gt_df[gt_df['y1'] <= lidar_bottom]\n",
        "    gt_df = gt_df[gt_df['y2'] >= lidar_top]\n",
        "\n",
        "    \n",
        "    column_names_2 = ['run_num','use_detector', 'max_cost', 'w0', 'w1', 'w2', 'total_associations', 'accuracy', 'precision', 'recall', 'total_possible_associations', 'true_pos', 'false_pos', 'false_neg']\n",
        "    #test_results = pd.read_csv('test_runs.csv')\n",
        "\n",
        "    #manual_test = [0, False, 1, 0.95, 0.05, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "    test_results = pd.DataFrame([manual_test], columns=column_names_2)\n",
        "\n",
        "\n",
        "    # create a class to access the kitti dataset\n",
        "    kitti_dataset = pykitti.raw(base_path='./data/', date=DATA_DATE, drive=RUN_NUMBER)\n",
        "\n",
        "    if USE_DETECTOR:\n",
        "        detector = CarDetectorTFV2()\n",
        "\n",
        "    first_frame = True\n",
        "\n",
        "    run_num = 0\n",
        "\n",
        "    for run_num in tqdm(range(len(test_results))):\n",
        "\n",
        "        total_possible_associations = 0\n",
        "        true_pos = 0\n",
        "        false_pos = 0\n",
        "        false_neg = 0\n",
        "\n",
        "        USE_DETECTOR = test_results.loc[(test_results['run_num'] == run_num)].use_detector.bool()\n",
        "\n",
        "\n",
        "\n",
        "        max_cost = np.float(test_results.loc[test_results['run_num'] == run_num].max_cost)\n",
        "        w0 = np.float(test_results.loc[test_results['run_num'] == run_num].w0)\n",
        "        w1 = np.float(test_results.loc[test_results['run_num'] == run_num].w1)\n",
        "        w2 = np.float(test_results.loc[test_results['run_num'] == run_num].w2)\n",
        "\n",
        "        weights = [w0, w1, w2]\n",
        "\n",
        "        column_names = ['frame', 'video_det_index', 'lidar_det_index', 'gt_index', 'lidar_dist', 'gt_dist', 'cost',\n",
        "                        'correct', 'max_cost', 'dist_thresh', 'w0', 'c0', 'w1', 'c1', 'w2', 'c2', ]\n",
        "        associations_record = pd.DataFrame([], columns=column_names)\n",
        "\n",
        "        for frame_num, frame_filename in tqdm(enumerate(kitti_dataset.cam2_files)):\n",
        "            new_frame = True\n",
        "            frame = cv2.imread(frame_filename)\n",
        "            success = frame.any()\n",
        "            frame_draw = frame.copy()\n",
        "            if not success:\n",
        "                print('no frame')\n",
        "                break\n",
        "\n",
        "            if first_frame:\n",
        "                # cv2.namedWindow('draw_frame')\n",
        "                # cv2.setMouseCallback('draw_frame', mouse_click)\n",
        "                if output_video_file:\n",
        "                    height, width, channels = frame.shape\n",
        "                    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "                    video_writer = cv2.VideoWriter(output_video_file, \n",
        "                                                   fourcc, \n",
        "                                                   20, \n",
        "                                                   (width, height))\n",
        "                first_frame = False\n",
        "\n",
        "\n",
        "            # get the ground truth values\n",
        "            gt_current_frame = gt_df.loc[gt_df['frame_number'] == frame_num]\n",
        "\n",
        "\n",
        "            # fill in the list of detections\n",
        "            bboxes = []\n",
        "            c = Costs()\n",
        "\n",
        "            if frame_num == 7:\n",
        "                a = 1\n",
        "\n",
        "\n",
        "            if USE_DETECTOR:\n",
        "                # get the video detections\n",
        "                \n",
        "                bbs, class_names, confidences = detector.detect(img=frame, return_class_scores=True)\n",
        "                for i, bb in enumerate(bbs):\n",
        "                    # ensure bb is inside the window\n",
        "                    bbs[i][0] = max(bb[0],0)\n",
        "                    bbs[i][1] = max(bb[1],0)\n",
        "                    bbs[i][2] = min(bb[2],cal.cal['X_RESOLUTION'])\n",
        "                    bbs[i][3] = min(bb[3],cal.cal['Y_RESOLUTION'])\n",
        "                n = len(class_names)\n",
        "                eliminate_flag = np.zeros(n,np.int)\n",
        "                for i, (bbox, class_name, confidence) in enumerate(zip(bbs, class_names, confidences)):\n",
        "                    # filter out bounding boxes that do not intersect with the lidar zone and are not vehicles\n",
        "                    if class_name not in ['car', 'truck', 'bus'] or confidence <= CONFIDENCE_THRESHOLD:\n",
        "                        eliminate_flag[i] = 1\n",
        "                    if (bbox[1] > lidar_bottom or bbox[3] < lidar_top or bbox[2] < lidar_left or bbox[0] > lidar_right):\n",
        "                        eliminate_flag[i] = 1\n",
        "                #remove the items from bbs, class_names and confidences\n",
        "                new_bbs = []; new_class_names = []; new_confidences = [];\n",
        "                for ii in range(n):\n",
        "                    if eliminate_flag[ii] == 0:\n",
        "                        new_bbs.append(bbs[ii])\n",
        "                        new_class_names.append(class_names[ii])\n",
        "                        new_confidences.append(confidences[ii])\n",
        "                bbs = new_bbs\n",
        "                class_names = new_class_names\n",
        "                confidences = new_confidences\n",
        "\n",
        "                n = len(class_names)\n",
        "                eliminate_flag = np.zeros(n,np.int)\n",
        "\n",
        "                #eliminate redundant detections - iou greater that 0.7 and different class_name\n",
        "                for ii in range(n):\n",
        "                    for jj in range(n):\n",
        "                        if ii < jj and c._iou(bbs[ii],bbs[jj]) >= 0.7 and class_names[ii] != class_names[jj]:\n",
        "                            eliminate_flag[jj] = 1\n",
        "\n",
        "                #remove the redundant items from bbs, class_names and confidences\n",
        "                new_bbs = []; new_class_names = []; new_confidences = [];\n",
        "                for ii in range(n):\n",
        "                    if eliminate_flag[ii] == 0:\n",
        "                        new_bbs.append(bbs[ii])\n",
        "                        new_class_names.append(class_names[ii])\n",
        "                        new_confidences.append(confidences[ii])\n",
        "                bbs = new_bbs\n",
        "                class_names = new_class_names\n",
        "                confidences = new_confidences\n",
        "                # only append what's left\n",
        "                for i, (bbox, class_name, confidence) in enumerate(zip(bbs, class_names, confidences)):\n",
        "                    bboxes.append(bbox)\n",
        "\n",
        "            else:\n",
        "                # get the bounding boxes from the ground truth data\n",
        "                gt_dist = []\n",
        "                for i, gt in enumerate(gt_current_frame.values):\n",
        "                    bboxes.append([gt[1], gt[2], gt[3], gt[4]])\n",
        "                    gt_dist.append(gt[6])\n",
        "                    total_possible_associations += 1\n",
        "\n",
        "            video_detections = []\n",
        "            if bboxes is not None and len(bboxes) > 0:\n",
        "                for i, bb in enumerate(bboxes):\n",
        "                    det = Detection()\n",
        "                    det.bbox = np.array([bb[0], bb[1], bb[2], bb[3]])\n",
        "                    det.frame_id = frame_num\n",
        "                    video_detections.append(det)\n",
        "\n",
        "\n",
        "            # get the lidar values\n",
        "            lidar_vals = m16.loc[m16['frame'] == frame_num-video_frame_lag]\n",
        "\n",
        "            lidar_detections = []\n",
        "            for ii in range(len(lidar_vals)):\n",
        "                if lidar_vals.iloc[ii,5] >= 30 and lidar_vals.iloc[ii,5] <= 140:\n",
        "                    lidar_detection = LIDAR_detection(frame_num,int(lidar_vals.iloc[ii,4]),lidar_vals.iloc[ii,5],lidar_vals.iloc[ii,6])\n",
        "\n",
        "                    lidar_detections.append(lidar_detection)\n",
        "\n",
        "\n",
        "\n",
        "            # perform the associations task\n",
        "            if len(lidar_detections) > 0 and len(video_detections) > 0:\n",
        "                associations = []\n",
        "                costs = Costs()\n",
        "\n",
        "                # total_cost(i,j) = w_0 * cost_function_0(i,j) + w_1 * cost_function_1(i,j) + .. + w_n * cost_function_n(i,j)\n",
        "                cost_functions = {costs.dist_between_centroids: weights[0],\n",
        "                                  costs.dist_lidar_to_y2estimate: weights[1],\n",
        "                                  costs.inverse_intersection_over_union: weights[2]}\n",
        "\n",
        "                a = Association()\n",
        "\n",
        "                # enter the video_detections and lidar_detections lists into the kwargs dictionary\n",
        "                kwargs = {'video_detections': video_detections, 'lidar_detections': lidar_detections}\n",
        "\n",
        "                # evaluate the costs array by passing the cost_functions dictionary and the kwargs dictionary to the evaluate_costs method\n",
        "                costs, cost_components = a.evaluate_cost(cost_functions, **kwargs)\n",
        "\n",
        "                original_costs = costs.copy()\n",
        "\n",
        "                c_shape = np.shape(costs)\n",
        "                rows = c_shape[0]\n",
        "                cols = c_shape[1]\n",
        "                if rows <= cols:\n",
        "                    assignments = a.compute_munkres(costs)\n",
        "#                     costs_T = np.transpose(costs)\n",
        "#                     import pdb; pdb.set_trace()\n",
        "#                     assignments_T = a.compute_munkres(costs_T)\n",
        "#                     assignments = []\n",
        "\n",
        "#                     for i, assignment in enumerate(assignments_T):\n",
        "#                         assignments.append((assignment[1],assignment[0]))\n",
        "\n",
        "                if len(assignments) != min(len(video_detections), len(lidar_detections)):\n",
        "                    a = 1\n",
        "                #determine if the associations are correct\n",
        "\n",
        "\n",
        "                if USE_DETECTOR:\n",
        "                    for i, gt in enumerate(gt_current_frame.values):\n",
        "                        total_possible_associations += 1\n",
        "                        bb_gt = [gt[1], gt[2], gt[3], gt[4]]\n",
        "                        for j, assignment in enumerate(assignments):\n",
        "                            if original_costs[assignment[0], assignment[1]] < max_cost:\n",
        "                                bb_v = video_detections[assignment[0]].bbox\n",
        "                                iou = c._iou(bb_gt, bb_v)\n",
        "                                if iou > 0:\n",
        "                                    dist_diff = abs(lidar_detections[assignment[1]].dist - gt[6])\n",
        "                                    if dist_diff < dist_thresh * gt[6]:\n",
        "                                        new_row = [frame_num, assignment[0], assignment[1], i,\n",
        "                                                   lidar_detections[assignment[1]].dist, gt[6],\n",
        "                                                   original_costs[assignment[0], assignment[1]], 'True', max_cost,\n",
        "                                                   dist_thresh, weights[0],\n",
        "                                                   cost_components[0][assignment[0], assignment[1]], weights[1],\n",
        "                                                   cost_components[1][assignment[0], assignment[1]], weights[2],\n",
        "                                                   cost_components[2][assignment[0], assignment[1]]]\n",
        "                                        true_pos += 1\n",
        "                                    else:\n",
        "                                        new_row = [frame_num, assignment[0], assignment[1], i,\n",
        "                                                   lidar_detections[assignment[1]].dist, gt[6],\n",
        "                                                   original_costs[assignment[0], assignment[1]], 'False', max_cost,\n",
        "                                                   dist_thresh, weights[0],\n",
        "                                                   cost_components[0][assignment[0], assignment[1]], weights[1],\n",
        "                                                   cost_components[1][assignment[0], assignment[1]], weights[2],\n",
        "                                                   cost_components[2][assignment[0], assignment[1]]]\n",
        "                                        false_pos += 1\n",
        "                                    row_num = len(associations_record)\n",
        "                                else:\n",
        "                                    new_row = [frame_num, assignment[0], assignment[1], i,\n",
        "                                               lidar_detections[assignment[1]].dist, gt[6],\n",
        "                                               original_costs[assignment[0], assignment[1]], 'false_neg_iouzero', max_cost,\n",
        "                                               dist_thresh, weights[0],\n",
        "                                               cost_components[0][assignment[0], assignment[1]], weights[1],\n",
        "                                               cost_components[1][assignment[0], assignment[1]], weights[2],\n",
        "                                               cost_components[2][assignment[0], assignment[1]]]\n",
        "                                    false_neg += 1\n",
        "                            else:\n",
        "                                new_row = [frame_num, assignment[0], assignment[1], i,\n",
        "                                           lidar_detections[assignment[1]].dist, gt[6],\n",
        "                                           original_costs[assignment[0], assignment[1]], 'false_neg_max_cost', max_cost,\n",
        "                                           dist_thresh, weights[0],\n",
        "                                           cost_components[0][assignment[0], assignment[1]], weights[1],\n",
        "                                           cost_components[1][assignment[0], assignment[1]], weights[2],\n",
        "                                           cost_components[2][assignment[0], assignment[1]]]\n",
        "                                false_neg += 1\n",
        "                            associations_record.loc[row_num] = new_row\n",
        "\n",
        "                else:\n",
        "                    cols = ['frame', 'video_det_index', 'lidar_det_index', 'gt_index', 'lidar_dist', 'gt_dist', 'cost', 'correct']\n",
        "                    for assignment in assignments:\n",
        "                        if original_costs[assignment[0],assignment[1]] < max_cost:\n",
        "                            dist_diff = abs(lidar_detections[assignment[1]].dist - gt_dist[assignment[0]])\n",
        "                            if dist_diff < dist_thresh * gt_dist[assignment[0]]:\n",
        "                                new_row = [frame_num, assignment[0], assignment[1], i, lidar_detections[assignment[1]].dist,\n",
        "                                           gt_dist[assignment[0]], original_costs[assignment[0], assignment[1]], 'True', max_cost, dist_thresh,\n",
        "                                           weights[0], cost_components[0][assignment[0], assignment[1]], weights[1],\n",
        "                                           cost_components[1][assignment[0], assignment[1]], weights[2],\n",
        "                                           cost_components[2][assignment[0], assignment[1]]]\n",
        "                                true_pos += 1\n",
        "                            else:\n",
        "                                new_row = [frame_num, assignment[0], assignment[1], i, lidar_detections[assignment[1]].dist,\n",
        "                                           gt_dist[assignment[0]], original_costs[assignment[0], assignment[1]], 'False', max_cost, dist_thresh,\n",
        "                                           weights[0], cost_components[0][assignment[0], assignment[1]], weights[1],\n",
        "                                           cost_components[1][assignment[0], assignment[1]], weights[2],\n",
        "                                           cost_components[2][assignment[0], assignment[1]]]\n",
        "                                false_pos += 1\n",
        "                            row_num = len(associations_record)       \n",
        "                        else:\n",
        "                            new_row = [frame_num, assignment[0], assignment[1], i, lidar_detections[assignment[1]].dist,\n",
        "                                       gt_dist[assignment[0]], original_costs[assignment[0], assignment[1]], 'false_neg_maxcost',\n",
        "                                       max_cost, dist_thresh,\n",
        "                                       weights[0], cost_components[0][assignment[0], assignment[1]], weights[1],\n",
        "                                       cost_components[1][assignment[0], assignment[1]], weights[2],\n",
        "                                       cost_components[2][assignment[0], assignment[1]]]\n",
        "                            false_neg += 1\n",
        "                        associations_record.loc[row_num] = new_row\n",
        "\n",
        "            #display the frame once if not PAUSE; continuously if PAUSE\n",
        "            while new_frame or PAUSE:\n",
        "                new_frame = False # only go through once unless PAUSE\n",
        "\n",
        "                # draw vertical line in center of image\n",
        "                cv2.line(frame_draw, (int(cal.cal['X_CENTER']), 0), (int(cal.cal['X_CENTER']), int(frame_draw.shape[0])),\n",
        "                         (255, 0, 255), 1)\n",
        "                cv2.line(frame_draw, (0, int(cal.cal['Y_HORIZON'])), (int(frame_draw.shape[1]), int(cal.cal['Y_HORIZON'])),\n",
        "                         (255, 0, 255), 1)\n",
        "                cv2.putText(frame_draw, 'frame: {0:0.0f}'.format(frame_num), (0, 25), 1, 2, (0, 0, 255), 2)\n",
        "\n",
        "                if DISP_DET: # show video detections in green\n",
        "                    for video_detection in video_detections:\n",
        "                        cv2.rectangle(frame_draw, (int(bb[0]), int(bb[1])), (int(bb[2]), int(bb[3])), (0, 255, 0), 2)\n",
        "\n",
        "                if DISP_LIDAR: # show lidar ideal bounding boxes in yellow\n",
        "                    for lidar_detection in lidar_detections:\n",
        "                        lidar_dist = lidar_detection.dist\n",
        "                        bb = lidar_detection.bb\n",
        "                        cv2.rectangle(img=frame_draw, pt1=(int(bb[0]), int(bb[1])), pt2=(int(bb[2]), int(bb[3])),\n",
        "                                      color=(0, 255, 255), thickness=2)\n",
        "                        cv2.putText(frame_draw, '{0:0.2f}'.format(lidar_dist), (int(bb[0]), int(bb[1])), 1, 1, (0, 0, 255), 2)\n",
        "\n",
        "                if DISP_ASSOC: # show associations in blue and red connected by a yellow line\n",
        "                    if len(lidar_detections) > 0 and len(video_detections) > 0:\n",
        "                        for assignment in assignments:\n",
        "                            if original_costs[assignment[0],assignment[1]] <= max_cost:\n",
        "                                bb_v = video_detections[assignment[0]].bbox\n",
        "                                dist_est = float(video_detections[assignment[0]].dist_est_y2[assignment[1]])\n",
        "                                lidar_dist = lidar_detections[assignment[1]].dist\n",
        "                                cv2.rectangle(img=frame_draw, pt1=(int(bb_v[0]),int(bb_v[1])), pt2=(int(bb_v[2]),int(bb_v[3])), color=(255,0,0), thickness=2)\n",
        "        #                        cv2.putText(frame_draw, '{0:0.0f}'.format(assignment[0]), (int(bb_v[0]),int(bb_v[1])), 1, 1, (255, 0, 0), 2)\n",
        "        #                        cv2.putText(frame_draw, '{0:0.2f}'.format(dist_est), (int(bb_v[0]-30), int(bb_v[3]+25)), 1, 1, (255, 0, 0), 2)\n",
        "                                bb_l = lidar_detections[assignment[1]].bb\n",
        "                                cv2.rectangle(img=frame_draw, pt1=(int(bb_l[0]),int(bb_l[1])), pt2=(int(bb_l[2]),int(bb_l[3])), color=(0,0,255), thickness=2)\n",
        "        #                        cv2.putText(frame_draw, '{0:0.0f}'.format(assignment[1]), (int(bb_l[0]),int(bb_l[1])), 1, 1, (0, 0, 255), 2)\n",
        "                                cv2.putText(frame_draw, '{0:0.2f}'.format(lidar_dist), (int(bb_l[0]),int(bb_l[3])+25), 1, 1, (0, 0, 255), 2)\n",
        "                                cv2.line(img=frame_draw, pt1=(int(bb_v[0]),int(bb_v[1])), pt2=(int(bb_l[0]),int(bb_l[1])), color=(0,255,255), thickness=2)\n",
        "\n",
        "                if DISP_ZONES: # show the lidar zone boundaries in black\n",
        "                    y1 = int(cal.SEG_TO_PIXEL_TOP)\n",
        "                    y2 = int(cal.SEG_TO_PIXEL_BOTTOM)\n",
        "                    for i in range(16):\n",
        "                        x = int(cal.SEG_TO_PIXEL_LEFT[i])\n",
        "                        cv2.line(frame_draw, (x, y1), (x, y2), (0, 0, 0), thickness=1)\n",
        "                        cv2.line(frame_draw, (x - 5, y1), (x + 5, y1), (0, 0, 0), thickness=1)\n",
        "                        cv2.line(frame_draw, (x - 5, y2), (x + 5, y2), (0, 0, 0), thickness=1)\n",
        "\n",
        "                    x = int(cal.SEG_TO_PIXEL_RIGHT[i])\n",
        "                    cv2.line(frame_draw, (x, y1), (x, y2), (0, 0, 0), thickness=1)\n",
        "                    cv2.line(frame_draw, (x - 5, y1), (x + 5, y1), (0, 0, 0), thickness=1)\n",
        "                    cv2.line(frame_draw, (x - 5, y2), (x + 5, y2), (0, 0, 0), thickness=1)\n",
        "\n",
        "                if DISP_TRUTH: # show the ground truth bboxes and dist\n",
        "                    for i in range(len(gt_current_frame)):\n",
        "                        x1, y1, x2, y2, label, dist = gt_current_frame.iloc[i,1:]\n",
        "                        if not (x2 < lidar_left or x1 > lidar_right or y1 > lidar_bottom or y2 < lidar_top):\n",
        "                            cv2.rectangle(img=frame_draw, pt1=(int(x1), int(y1)), pt2=(int(x2), int(y2)),color=(0, 255, 0), thickness=2)\n",
        "                            cv2.putText(frame_draw, '{0:0.1f}'.format(dist), (int(x1), int(y1)), 1, 1,(0, 255, 0), 2)\n",
        "\n",
        "                # show the nouse coordinates\n",
        "                if x_pixel >= 0 and no_mouse_click_count > 0:\n",
        "                    cv2.putText(frame_draw, '({0:0.0f}, {1:0.0f})'.format(x_pixel, y_pixel),\n",
        "                                (cal.cal['X_RESOLUTION'] - 120, 15), 1, 1, (0, 0, 255), 2)\n",
        "                    no_mouse_click_count -= 1\n",
        "\n",
        "                if DISP_RESULTS:\n",
        "                    pass\n",
        "\n",
        "                cv2.putText(frame_draw, 'frame: {0:0.0f}'.format(frame_num), (0, 25), 1, 2, (0, 0, 255), 2)\n",
        "                # cv2.imshow('draw_frame', frame_draw)\n",
        "                if video_writer:\n",
        "                    video_writer.write(frame_draw)\n",
        "\n",
        "                if SLOW:\n",
        "                    key = cv2.waitKey(1000) & 0xFF\n",
        "                else:\n",
        "                    key = cv2.waitKey(30) & 0xFF\n",
        "\n",
        "                if frame_num == 212:\n",
        "                    a = 1\n",
        "\n",
        "                if key == ord('q') or key == 27:\n",
        "                    exit(0)\n",
        "                if key == ord('p') or key == ord('P'):\n",
        "                    PAUSE = not PAUSE\n",
        "                if key == ord('l') or key == ord('L'):\n",
        "                    DISP_LIDAR = not DISP_LIDAR\n",
        "                if key == ord('d') or key == ord('D'):\n",
        "                    DISP_DET = not DISP_DET\n",
        "                if key == ord('a') or key == ord('A'):\n",
        "                    DISP_ASSOC = not DISP_ASSOC\n",
        "                if key == ord('s') or key == ord('S'):\n",
        "                    SLOW = not SLOW\n",
        "                if key == ord('z') or key == ord('Z'):\n",
        "                    DISP_ZONES = not DISP_ZONES\n",
        "                if key == ord('t') or key == ord('T'):\n",
        "                    DISP_TRUTH = not DISP_TRUTH\n",
        "\n",
        "        false_neg = total_possible_associations - (true_pos + false_pos)\n",
        "        accuracy = true_pos / total_possible_associations\n",
        "        if true_pos + false_pos > 0:\n",
        "            precision = true_pos / (true_pos + false_pos)\n",
        "        else:\n",
        "            precision = np.nan\n",
        "\n",
        "        if true_pos + false_neg > 0:\n",
        "            recall = true_pos / (true_pos + false_neg)\n",
        "        else:\n",
        "            recall = np.nan\n",
        "\n",
        "        now = datetime.datetime.now()\n",
        "        filename = 'results_{0:04d}.csv'.format(run_num)\n",
        "\n",
        "        associations_record.to_csv(filename, index=False)\n",
        "\n",
        "        print('run: {0:0.0f}, accy: {1:0.3f}, prec: {2:0.3f}, recall: {3:0.3f}, total_assoc: {4:0.0f}, total_poss_assoc: {5:0.0f}, true_pos: {6:0.0f}, '\n",
        "              'false_pos: {7:0.0f}, false_neg: {8:0.0f}, use_detector:{9:}, max_cost: {10:0.3f}, w0: {11:0.3f}, w1: {12:0.3f}, '\n",
        "              'w2: {13:0.3f}'.format(run_num, accuracy, precision, recall, len(associations_record), total_possible_associations, true_pos, false_pos, false_neg, str(USE_DETECTOR), max_cost, weights[0], weights[1], weights[2]))\n",
        "\n",
        "        #column_names_2 = ['run_num', 'max_cost', 'w0', 'w1', 'w2', 'total_associations', 'accuracy',\n",
        "        #                  'total_possible_associations', 'true_pos', 'false_pos', 'false_neg']\n",
        "\n",
        "        test_results.iloc[run_num,6] = len(associations_record)\n",
        "        test_results.iloc[run_num,7] = accuracy\n",
        "        test_results.iloc[run_num,8] = precision\n",
        "        test_results.iloc[run_num,9] = recall\n",
        "        test_results.iloc[run_num,10] = total_possible_associations\n",
        "        test_results.iloc[run_num,11] = true_pos\n",
        "        test_results.iloc[run_num,12] = false_pos\n",
        "        test_results.iloc[run_num,13] = false_neg\n",
        "\n",
        "    filename = 'test_results_' + str(now) + '.csv'\n",
        "    test_results.to_csv(filename)\n",
        "    if video_writer:\n",
        "        video_writer.release()\n",
        "    # cv2.waitKey(1)\n",
        "    # cv2.destroyAllWindows()\n",
        "    # cv2.waitKey(1)\n",
        "\n",
        "    print('Run Complete!')\n",
        "    \n",
        "    return test_results\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qhhiQINT8NU",
        "colab_type": "text"
      },
      "source": [
        "## Run Tests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXMpoSaJDZan",
        "colab_type": "text"
      },
      "source": [
        "### Testing Definitions\n",
        "\n",
        "The following definitions are used to evaluate the accuracy of the association process:\n",
        "\n",
        "\n",
        "__True Positive:__\n",
        "\n",
        "If: an association has a video detection bounding box that intersects with a Ground Truth bounding box __AND__ has a cost value less than the hyper-parameter max_cost __AND__ has a lidar distance value __within__ the hyper-parameter __dist_thresh__ (percentage) of the Ground Truth distance \n",
        "\n",
        "Then: it is labeled a __True Positive__ association (true_pos)\n",
        "\n",
        ">These __True Positive__ associations are the correct associations with a video detection and lidar reading that is related with high confidence to a labeled Ground Truth object\n",
        "\n",
        "__False Positive - Video Detector:__\n",
        "\n",
        "If: an association is made with a video detection bounding box that does not intersect a ground truth bounding box \n",
        "\n",
        "Then: the association is labeled a __False Positive - Video Detector__\n",
        ">These __False Positive - Video Detector__ associations are errors caused by the video detector rather than the association process so they do not effect the association accuracy\n",
        "\n",
        "__False Positive:__\n",
        "\n",
        "If: an association is made that has a cost value greater than the hyper-parameter max_cost __OR__ has a lidar distance value outside of the hyper-parameter __dist_thresh__ (percentage) of the Ground Truth distance \n",
        "\n",
        "Then: the association is labeled a __False Positive:__ (false_pos)\n",
        "\n",
        ">These __False Positive:__ associations are the incorrect associations with a video detection and lidar reading that intersects a Ground Truth object but fails either the max_cost of dist_thresh tests. Note that the Munkres Algorithm always returns the association pairs that have a global minimum cost. Some of these associations may distances that are too far away from the ground truth or may just barely intersect the bounding box and need to be rejected using hyper-parameters.\n",
        "\n",
        "__False Negative:__\n",
        "\n",
        "If: all the associations have been processed and a ground truth object has not been labeled as either a True Positive or a False Positive using the rules above\n",
        "\n",
        "Then: the ground truth object is labeled as a __False Negative__ (false_neg)\n",
        "\n",
        ">These __False Negative__ associations are missing associations that failed to be made either due to the lack of a video detection that intersects with a Ground Truth object or the lack of a lidar detection that intersects with the video detection. The most common of these two faults is the missing video detection. To evaluate the magnitude of the missing video detections, the number of false negatives can be compared using a USE_DETECTION = False hyper-parameter on the association algorithm.\n",
        "\n",
        "\n",
        "__True Negative:__\n",
        "\n",
        "True Negatives are not evaluated in this algorithm because they are not applicable.\n",
        "\n",
        "### Calculating Accuracy, Precision and Recall\n",
        "\n",
        "In addition, the following equations are used to calculate the accuracy, precision and recall of a run.\n",
        "\n",
        "\\begin{equation*}\n",
        "Accuracy = \\frac{True Positives}{(True Positives + False Positives + False Negatives)}\n",
        "\\end{equation*}\n",
        "\n",
        "\\begin{equation*}\n",
        "Precision = \\frac{True Positives}{(True Positives + False Positives)}\n",
        "\\end{equation*}\n",
        "\n",
        "\\begin{equation*}\n",
        "Recall = \\frac{True Positives}{(True Positives + False Negatives)}\n",
        "\\end{equation*}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uz0krStXiqAy",
        "colab_type": "text"
      },
      "source": [
        "### Run Test #1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwVydWe0DZan",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#column_names_2 = ['run_num','use_detector', 'max_cost', 'w0', 'w1', 'w2', 'total_associations', 'accuracy', 'precision', 'recall', 'total_possible_associations', 'true_pos', 'false_pos', 'false_neg']\n",
        "manual_test1 = [0, False, 1, 0.95, 0.05, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "# import pdb; pdb.set_trace()\n",
        "output_video_file=\"test_results1.mp4\"\n",
        "test_results1 = run_association_test(manual_test1, output_video_file=output_video_file)\n",
        "test_results1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZViC57E1DZas",
        "colab_type": "text"
      },
      "source": [
        "### Download video of test results\n",
        "Execute the following cell to download the video from the test run "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whyyKkA4iHK4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "download_file(output_video_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYdpG-4yizjE",
        "colab_type": "text"
      },
      "source": [
        "### Run Test #2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "x7Iaq5E2DZaw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "manual_test2 = [0, True, 1, 0.95, 0.05, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "output_video_file2=\"test_results2.mp4\"\n",
        "test_results2 = run_association_test(manual_test2, output_video_file=output_video_file)\n",
        "test_results2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0F7sLQxDZa0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "download_file(output_video_file2)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}